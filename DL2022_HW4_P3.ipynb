{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b78ca80",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## HW4 - Problem 3\n",
    "\n",
    "Name: Amin Robatian\n",
    "\n",
    "Student Number: 400301075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f344545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Packages\n",
    "\n",
    "#!pip install transformers\n",
    "#!pip install -q hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb18a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dllabsharif/Robatian/Persian_poems_corpus/normalized\n"
     ]
    }
   ],
   "source": [
    "# Download Dataset from Github\n",
    "\n",
    "#!git clone https://github.com/aminrobatian/Persian_poems_corpus.git\n",
    "\n",
    "%cd ./Persian_poems_corpus/normalized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a76b2789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "%matplotlib inline\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torchsummary import summary\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "import hazm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2afe41",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b398e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PoetName</th>\n",
       "      <th>Nameinfolders</th>\n",
       "      <th>PoetID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vahshi Bafqi</td>\n",
       "      <td>vahshi_norm.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jami</td>\n",
       "      <td>jami_norm.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asadi Tusi</td>\n",
       "      <td>asadi_norm.txt</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Attar of Nishapur</td>\n",
       "      <td>attar_norm.txt</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mohammad Taqi Bahar</td>\n",
       "      <td>bahar_norm.txt</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Farrokhi Yazdi</td>\n",
       "      <td>farrokhi_norm.txt</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ferdowsi</td>\n",
       "      <td>ferdousi_norm.txt</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Shah Nimatullah Wali</td>\n",
       "      <td>shahnematollah_norm.txt</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Khwaju Kermani</td>\n",
       "      <td>khajoo_norm.txt</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rumi</td>\n",
       "      <td>moulavi_norm.txt</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               PoetName            Nameinfolders  PoetID\n",
       "0          Vahshi Bafqi          vahshi_norm.txt       0\n",
       "1                  Jami            jami_norm.txt       1\n",
       "2            Asadi Tusi           asadi_norm.txt       2\n",
       "3     Attar of Nishapur           attar_norm.txt       3\n",
       "4   Mohammad Taqi Bahar           bahar_norm.txt       4\n",
       "5        Farrokhi Yazdi        farrokhi_norm.txt       5\n",
       "6              Ferdowsi        ferdousi_norm.txt       6\n",
       "7  Shah Nimatullah Wali  shahnematollah_norm.txt       7\n",
       "8        Khwaju Kermani          khajoo_norm.txt       8\n",
       "9                  Rumi         moulavi_norm.txt       9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poets = [['Vahshi Bafqi', 'vahshi_norm.txt', 0],\n",
    "         ['Jami', 'jami_norm.txt', 1],\n",
    "         ['Asadi Tusi', 'asadi_norm.txt', 2],\n",
    "         ['Attar of Nishapur', 'attar_norm.txt', 3],\n",
    "         ['Mohammad Taqi Bahar', 'bahar_norm.txt', 4],\n",
    "         ['Farrokhi Yazdi', 'farrokhi_norm.txt', 5],\n",
    "         ['Ferdowsi', 'ferdousi_norm.txt', 6],\n",
    "         ['Shah Nimatullah Wali', 'shahnematollah_norm.txt', 7],\n",
    "         ['Khwaju Kermani', 'khajoo_norm.txt', 8],\n",
    "         ['Rumi', 'moulavi_norm.txt', 9]]\n",
    "poets_df = pd.DataFrame(poets, columns=['PoetName', 'Nameinfolders', 'PoetID'])\n",
    "poets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b747829e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beyt</th>\n",
       "      <th>poet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>اگر در سخن موی کافد همی [SEP] به تاریکی اندر ب...</td>\n",
       "      <td>Ferdowsi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>آن شفاعت و آن دعا نه از رحم خود [SEP] می کند آ...</td>\n",
       "      <td>Rumi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ز بس طپانچه که هر شب بروی برزدمی [SEP] بزور بو...</td>\n",
       "      <td>Farrokhi Yazdi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>یکی باشد در آنجا هر چه بینی [SEP] اگر تو مرد ر...</td>\n",
       "      <td>Attar of Nishapur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>تو هم ای دایه زبن هنر بشکن [SEP] دل ما سوختی د...</td>\n",
       "      <td>Mohammad Taqi Bahar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280251</th>\n",
       "      <td>اگر تو پاکباز آیی درین راه [SEP] چو ما بیشک رس...</td>\n",
       "      <td>Attar of Nishapur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280252</th>\n",
       "      <td>ببیند که آن دو دلاور کیند [SEP] بران کوه سر بر...</td>\n",
       "      <td>Ferdowsi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280253</th>\n",
       "      <td>وآنک او آن نور را بینا بود [SEP] شرح او کی کار...</td>\n",
       "      <td>Rumi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280254</th>\n",
       "      <td>همه عشقست اگر خود بازیابی [SEP] ز عشق اینجا حق...</td>\n",
       "      <td>Attar of Nishapur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280255</th>\n",
       "      <td>بگو کامشب چه میخواهی بگویم [SEP] که بیشک سرور ...</td>\n",
       "      <td>Attar of Nishapur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280256 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     beyt                 poet\n",
       "0       اگر در سخن موی کافد همی [SEP] به تاریکی اندر ب...             Ferdowsi\n",
       "1       آن شفاعت و آن دعا نه از رحم خود [SEP] می کند آ...                 Rumi\n",
       "2       ز بس طپانچه که هر شب بروی برزدمی [SEP] بزور بو...       Farrokhi Yazdi\n",
       "3       یکی باشد در آنجا هر چه بینی [SEP] اگر تو مرد ر...    Attar of Nishapur\n",
       "4       تو هم ای دایه زبن هنر بشکن [SEP] دل ما سوختی د...  Mohammad Taqi Bahar\n",
       "...                                                   ...                  ...\n",
       "280251  اگر تو پاکباز آیی درین راه [SEP] چو ما بیشک رس...    Attar of Nishapur\n",
       "280252  ببیند که آن دو دلاور کیند [SEP] بران کوه سر بر...             Ferdowsi\n",
       "280253  وآنک او آن نور را بینا بود [SEP] شرح او کی کار...                 Rumi\n",
       "280254  همه عشقست اگر خود بازیابی [SEP] ز عشق اینجا حق...    Attar of Nishapur\n",
       "280255  بگو کامشب چه میخواهی بگویم [SEP] که بیشک سرور ...    Attar of Nishapur\n",
       "\n",
       "[280256 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(columns=['beyt', 'poet'])\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "  df = pd.read_csv(poets_df.Nameinfolders[i], header = None)\n",
    "    \n",
    "  lst = int(len(df)/2) * [' ']\n",
    "  df_beyt = pd.DataFrame(lst)\n",
    "\n",
    "  for k in range(int(len(df)/2)):\n",
    "    df_beyt.iloc[k] = df.iloc[2*k] + ' [SEP] ' + df.iloc[2*k+1]\n",
    "  \n",
    "\n",
    "  df_beyt[\"poet\"] = poets_df.PoetName[i]\n",
    "  df_beyt.columns = ['beyt', 'poet']\n",
    "  data = pd.concat([data, df_beyt], ignore_index=True, sort=False)\n",
    "\n",
    "\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4603b62e",
   "metadata": {},
   "source": [
    "## Train, Validation, Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62823070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, Validation, Test split (80:10:10)\n",
      "Train Length:           224,204 \n",
      "Validation Length:       28,026 \n",
      "Test Length:            28,026\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val, df_test = np.split(data.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(data)), int(.9*len(data))])\n",
    "\n",
    "\n",
    "print(f\"Train, Validation, Test split (80:10:10)\")\n",
    "print(f\"Train Length: {len(df_train):>17,} \\nValidation Length: \\\n",
    "      {len(df_val):,} \\nTest Length: {len(df_test):>17,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53426081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Beyt Len by Words: 6 \tMax Beyt Len by Words: 29\n"
     ]
    }
   ],
   "source": [
    "# calculate the length of comments based on their words\n",
    "data['beyt_len_by_words'] = data['beyt'].apply(lambda t: len(hazm.word_tokenize(t)))\n",
    "\n",
    "min_max_len = data[\"beyt_len_by_words\"].min(), data[\"beyt_len_by_words\"].max()\n",
    "print(f'Min Beyt Len by Words: {min_max_len[0]} \\tMax Beyt Len by Words: {min_max_len[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4755415",
   "metadata": {},
   "source": [
    "# Part (A) - Pretrained BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493dafc4",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cc08a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fef1591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: \n",
      "tensor([[    2,  3064,  2786,  4402, 12378,  6941,  2013, 27131,     4,  2789,\n",
      "         10973, 11840, 65322,  2013, 27131,     4,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "\n",
      "token_type_ids: \n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "attention_mask: \n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "Formatted Sequence: \n",
      "[CLS] اگر در سخن موی کافد همی [SEP] به تاریکی اندر ببافد همی [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "example_text = df_train.beyt[0]\n",
    "bert_input = tokenizer(example_text,padding='max_length', max_length = 32, \n",
    "                       truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "print(f\"input_ids: \\n{bert_input['input_ids']}\\n\")\n",
    "print(f\"token_type_ids: \\n{bert_input['token_type_ids']}\\n\")\n",
    "print(f\"attention_mask: \\n{bert_input['attention_mask']}\\n\")\n",
    "\n",
    "example_text = tokenizer.decode(bert_input.input_ids[0])\n",
    "\n",
    "print(f\"Formatted Sequence: \\n{example_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22687f56",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e3aadb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Vahshi Bafqi': 0,\n",
       " 'Jami': 1,\n",
       " 'Asadi Tusi': 2,\n",
       " 'Attar of Nishapur': 3,\n",
       " 'Mohammad Taqi Bahar': 4,\n",
       " 'Farrokhi Yazdi': 5,\n",
       " 'Ferdowsi': 6,\n",
       " 'Shah Nimatullah Wali': 7,\n",
       " 'Khwaju Kermani': 8,\n",
       " 'Rumi': 9}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dict(zip(poets_df.PoetName, poets_df.PoetID))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7b22bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersianPoemsDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Create a PyTorch dataset for Persian poems corpus. \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = [labels[label] for label in df['poet']]\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 32, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['beyt']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b271ee5",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aba71fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HooshvareLab/bert-fa-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(100000, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ParsBERT = BertModel.from_pretrained('HooshvareLab/bert-fa-base-uncased')\n",
    "print(ParsBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ee6d45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trainable Parameters Before Freezing the Model: 162,841,344\n",
      "Number of Trainable Parameters After Freezing the Model: 0\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in ParsBERT.parameters() if p.requires_grad)\n",
    "print(f\"Number of Trainable Parameters Before Freezing the Model: {pytorch_total_params:,}\")\n",
    "\n",
    "for param in ParsBERT.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in ParsBERT.parameters() if p.requires_grad)\n",
    "print(f\"Number of Trainable Parameters After Freezing the Model: {pytorch_total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23a10c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = ParsBERT\n",
    "        self.linear = nn.Linear(768, 10)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        linear_output = self.linear(pooled_output)\n",
    "\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9605422e",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24b4996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs, criterion, optimizer):\n",
    "\n",
    "    train, val = PersianPoemsDataset(train_data), PersianPoemsDataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=32)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {(100 * total_acc_train / len(train_data)): .2f}% \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {(100 * total_acc_val / len(val_data)): .2f}%')\n",
    "            \n",
    "            avg_loss = total_loss_val / len(val_data)\n",
    "            \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0353e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = BertClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed966815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7007/7007 [01:09<00:00, 101.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.055                 | Train Accuracy:  41.79%                 | Val Loss:  0.052                 | Val Accuracy:  44.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7007/7007 [01:08<00:00, 101.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.051                 | Train Accuracy:  45.75%                 | Val Loss:  0.050                 | Val Accuracy:  46.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7007/7007 [01:08<00:00, 101.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.049                 | Train Accuracy:  47.13%                 | Val Loss:  0.049                 | Val Accuracy:  47.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7007/7007 [01:08<00:00, 101.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.048                 | Train Accuracy:  47.95%                 | Val Loss:  0.048                 | Val Accuracy:  48.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7007/7007 [01:08<00:00, 101.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.048                 | Train Accuracy:  48.58%                 | Val Loss:  0.048                 | Val Accuracy:  48.37%\n",
      "/home/dllabsharif/Robatian\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "LR = 5e-5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr= LR)\n",
    "              \n",
    "avg_loss = train(model, df_train, df_val, LR, EPOCHS, criterion, optimizer)\n",
    "\n",
    "# Saving the Model\n",
    "%cd /home/dllabsharif/Robatian/\n",
    "torch.save(model, 'Pretrained_BERT.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4ed81f",
   "metadata": {},
   "source": [
    "## Evaluate Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b177f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = PersianPoemsDataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "              test_label = test_label.to(device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "\n",
    "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "              total_acc_test += acc\n",
    "            \n",
    "              output = output.argmax(dim=1).data.cpu().numpy()\n",
    "              y_pred.extend(output) # Save Prediction\n",
    "              test_label = test_label.data.cpu().numpy()\n",
    "              y_true.extend(test_label) # Save Truth\n",
    "            \n",
    "            \n",
    "    \n",
    "    print(f'Avg Loss: {avg_loss:>6f}\\n')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {(100*accuracy):>0.2f}%\\n')\n",
    "    f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    print(f'F1 score: {(100*f1):>0.2f}%\\n')\n",
    "    # constant for classes\n",
    "    classes = [i for i in labels]  \n",
    "    # Build confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) * 100, index = [i for i in classes])\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(round(df_cm, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91de0d",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7a26c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dllabsharif/Robatian\n",
      "Avg Loss: 0.047709\n",
      "\n",
      "Accuracy: 48.26%\n",
      "\n",
      "F1 score: 48.26%\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "                         0     1     2      3     4     5      6     7     8  \\\n",
      "Vahshi Bafqi          0.03  0.56  0.01   2.13  0.16  0.01   0.39  0.11  0.04   \n",
      "Jami                  0.00  2.63  0.01   6.75  0.33  0.02   1.70  0.17  0.02   \n",
      "Asadi Tusi            0.00  0.21  0.04   0.86  0.07  0.01   1.86  0.01  0.01   \n",
      "Attar of Nishapur     0.01  0.96  0.02  29.32  0.30  0.04   2.34  0.37  0.06   \n",
      "Mohammad Taqi Bahar   0.01  0.67  0.02   3.46  0.97  0.03   1.52  0.12  0.01   \n",
      "Farrokhi Yazdi        0.00  0.21  0.02   2.35  0.24  0.20   1.13  0.05  0.03   \n",
      "Ferdowsi              0.01  0.44  0.01   4.05  0.25  0.03  12.53  0.08  0.01   \n",
      "Shah Nimatullah Wali  0.01  0.24  0.00   3.41  0.05  0.01   0.17  1.31  0.04   \n",
      "Khwaju Kermani        0.00  0.21  0.00   2.21  0.08  0.01   0.17  0.13  0.19   \n",
      "Rumi                  0.00  0.38  0.01   6.65  0.22  0.04   0.90  0.22  0.03   \n",
      "\n",
      "                         9  \n",
      "Vahshi Bafqi          0.19  \n",
      "Jami                  0.39  \n",
      "Asadi Tusi            0.05  \n",
      "Attar of Nishapur     0.60  \n",
      "Mohammad Taqi Bahar   0.36  \n",
      "Farrokhi Yazdi        0.21  \n",
      "Ferdowsi              0.16  \n",
      "Shah Nimatullah Wali  0.16  \n",
      "Khwaju Kermani        0.15  \n",
      "Rumi                  1.05  \n"
     ]
    }
   ],
   "source": [
    "%cd /home/dllabsharif/Robatian/\n",
    "model = torch.load('Pretrained_BERT.pth')\n",
    "model.eval()\n",
    "\n",
    "evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ae5604",
   "metadata": {},
   "source": [
    "# Part (B) - BERT Fine-Tuning\n",
    "\n",
    "# 1.   SGD Optimizer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bd7246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('HooshvareLab/bert-fa-base-uncased')\n",
    "        self.linear = nn.Linear(768, 10)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        linear_output = self.linear(pooled_output)\n",
    "\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "408ba20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HooshvareLab/bert-fa-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_SGD = BertClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54b05540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7007/7007 [04:08<00:00, 28.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.045                 | Train Accuracy:  52.72%                 | Val Loss:  0.036                 | Val Accuracy:  61.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7007/7007 [04:09<00:00, 28.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.032                 | Train Accuracy:  65.29%                 | Val Loss:  0.030                 | Val Accuracy:  67.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7007/7007 [04:09<00:00, 28.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.027                 | Train Accuracy:  70.09%                 | Val Loss:  0.027                 | Val Accuracy:  70.58%\n",
      "/home/dllabsharif/Robatian\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "LR = 5e-5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model_SGD.parameters(), lr= LR, momentum=0.9)\n",
    "              \n",
    "avg_loss = train(model_SGD, df_train, df_val, LR, EPOCHS, criterion, optimizer)\n",
    "\n",
    "# Saving the Model\n",
    "%cd /home/dllabsharif/Robatian/\n",
    "torch.save(model_SGD, 'Fine_Tuned_BERT_SGD.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2200cbc",
   "metadata": {},
   "source": [
    "# SGD Optimizer Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d84d66ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dllabsharif/Robatian\n",
      "Avg Loss: 0.026916\n",
      "\n",
      "Accuracy: 70.64%\n",
      "\n",
      "F1 score: 70.64%\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "                         0     1     2      3     4     5      6     7     8  \\\n",
      "Vahshi Bafqi          0.35  1.23  0.01   0.80  0.49  0.10   0.04  0.11  0.24   \n",
      "Jami                  0.03  9.90  0.03   0.99  0.27  0.01   0.09  0.17  0.00   \n",
      "Asadi Tusi            0.01  0.44  0.52   0.33  0.14  0.06   1.53  0.00  0.00   \n",
      "Attar of Nishapur     0.11  1.99  0.02  28.12  0.36  0.13   0.34  0.54  0.16   \n",
      "Mohammad Taqi Bahar   0.09  1.66  0.02   0.71  2.96  0.41   0.30  0.14  0.16   \n",
      "Farrokhi Yazdi        0.05  0.39  0.02   0.33  0.62  2.40   0.13  0.05  0.08   \n",
      "Ferdowsi              0.00  0.59  0.31   0.59  0.22  0.11  15.65  0.00  0.00   \n",
      "Shah Nimatullah Wali  0.04  0.51  0.00   0.77  0.09  0.04   0.00  3.67  0.09   \n",
      "Khwaju Kermani        0.08  0.15  0.00   0.61  0.16  0.16   0.00  0.13  1.60   \n",
      "Rumi                  0.04  0.87  0.00   2.25  0.34  0.14   0.02  0.27  0.10   \n",
      "\n",
      "                         9  \n",
      "Vahshi Bafqi          0.24  \n",
      "Jami                  0.52  \n",
      "Asadi Tusi            0.07  \n",
      "Attar of Nishapur     2.23  \n",
      "Mohammad Taqi Bahar   0.73  \n",
      "Farrokhi Yazdi        0.37  \n",
      "Ferdowsi              0.09  \n",
      "Shah Nimatullah Wali  0.20  \n",
      "Khwaju Kermani        0.26  \n",
      "Rumi                  5.47  \n"
     ]
    }
   ],
   "source": [
    "%cd /home/dllabsharif/Robatian/\n",
    "model_SGD = torch.load('Fine_Tuned_BERT_SGD.pth')\n",
    "model_SGD.eval()\n",
    "\n",
    "evaluate(model_SGD, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7cd55c",
   "metadata": {},
   "source": [
    "# Part (B) - BERT Fine-Tuning\n",
    "\n",
    "# 2.   Adam Optimizer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b44fe267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HooshvareLab/bert-fa-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_Adam = BertClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "918cd284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7007/7007 [05:15<00:00, 22.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.024                 | Train Accuracy:  74.14%                 | Val Loss:  0.018                 | Val Accuracy:  80.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7007/7007 [05:16<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.013                 | Train Accuracy:  86.36%                 | Val Loss:  0.016                 | Val Accuracy:  81.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7007/7007 [05:16<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.007                 | Train Accuracy:  92.01%                 | Val Loss:  0.017                 | Val Accuracy:  82.75%\n",
      "/home/dllabsharif/Robatian\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "LR = 5e-5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model_Adam.parameters(), lr= LR)\n",
    "              \n",
    "avg_loss = train(model_Adam, df_train, df_val, LR, EPOCHS, criterion, optimizer)\n",
    "\n",
    "# Saving the Model\n",
    "%cd /home/dllabsharif/Robatian/\n",
    "torch.save(model_Adam, 'Fine_Tuned_BERT_Adam.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22395a73",
   "metadata": {},
   "source": [
    "# Adam Optimizer Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "649605e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dllabsharif/Robatian\n",
      "Avg Loss: 0.017121\n",
      "\n",
      "Accuracy: 82.89%\n",
      "\n",
      "F1 score: 82.89%\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "                         0      1     2      3     4     5      6     7     8  \\\n",
      "Vahshi Bafqi          1.55   0.60  0.01   0.49  0.44  0.11   0.02  0.10  0.10   \n",
      "Jami                  0.16  10.02  0.06   0.66  0.47  0.05   0.05  0.09  0.01   \n",
      "Asadi Tusi            0.02   0.10  1.89   0.11  0.04  0.02   0.92  0.00  0.00   \n",
      "Attar of Nishapur     0.18   0.54  0.06  30.56  0.50  0.09   0.15  0.29  0.14   \n",
      "Mohammad Taqi Bahar   0.10   0.54  0.11   0.46  4.91  0.25   0.24  0.06  0.09   \n",
      "Farrokhi Yazdi        0.02   0.05  0.04   0.15  0.43  3.47   0.07  0.01  0.09   \n",
      "Ferdowsi              0.04   0.05  0.61   0.16  0.05  0.00  16.64  0.00  0.00   \n",
      "Shah Nimatullah Wali  0.02   0.24  0.00   0.38  0.12  0.02   0.00  4.26  0.08   \n",
      "Khwaju Kermani        0.08   0.05  0.00   0.27  0.17  0.05   0.00  0.14  2.33   \n",
      "Rumi                  0.02   0.15  0.00   1.44  0.32  0.10   0.02  0.12  0.06   \n",
      "\n",
      "                         9  \n",
      "Vahshi Bafqi          0.20  \n",
      "Jami                  0.46  \n",
      "Asadi Tusi            0.02  \n",
      "Attar of Nishapur     1.51  \n",
      "Mohammad Taqi Bahar   0.41  \n",
      "Farrokhi Yazdi        0.10  \n",
      "Ferdowsi              0.01  \n",
      "Shah Nimatullah Wali  0.27  \n",
      "Khwaju Kermani        0.07  \n",
      "Rumi                  7.26  \n"
     ]
    }
   ],
   "source": [
    "%cd /home/dllabsharif/Robatian/\n",
    "model_Adam = torch.load('Fine_Tuned_BERT_Adam.pth')\n",
    "model_Adam.eval()\n",
    "\n",
    "evaluate(model_Adam, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4f3e0",
   "metadata": {},
   "source": [
    "# Part (C) - Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb5ba3",
   "metadata": {},
   "source": [
    "The **perplexity** of the model m is the exponential of its cross entropy:\n",
    "\n",
    "$Perplexity(m)=2^{-\\sum_{i=1}^{n}p(x_i)log_2 m(x_i)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be90b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model1, model2, test_data):\n",
    "\n",
    "    test = PersianPoemsDataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=100)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model1 = model1.cuda()\n",
    "        model2 = model2.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "            \n",
    "            test_label = test_label.to(device) # targets\n",
    "            \n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "            \n",
    "            # model outputs / logits\n",
    "            output1 = model1(input_id, mask)\n",
    "            output2 = model2(input_id, mask)\n",
    "            \n",
    "            # getting loss using cross entropy\n",
    "            loss1 = F.cross_entropy(output1, test_label)\n",
    "            loss2 = F.cross_entropy(output2, test_label)\n",
    "            \n",
    "            # calculating perplexity\n",
    "            perplexity1  = torch.exp(loss1)\n",
    "            perplexity2  = torch.exp(loss2)\n",
    "            \n",
    "            return loss1, loss2, perplexity1, perplexity2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f7375d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1, loss2, perplexity1, perplexity2 = perplexity(model, model_Adam, df_test)\n",
    "\n",
    "loss1 = loss1.cpu().numpy()\n",
    "loss2 = loss2.cpu().numpy()\n",
    "perplexity1 = perplexity1.cpu().numpy()\n",
    "perplexity2 = perplexity2.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c0cd9",
   "metadata": {},
   "source": [
    "# **Perplexity Before Fine-tuning:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "529f6677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Fine-tuning:\n",
      "PP = 6.1174\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before Fine-tuning:\")  \n",
    "print(f\"PP = {perplexity1:>0.4f}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928be051",
   "metadata": {},
   "source": [
    "# **Perplexity After Fine-tuning:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70e0afc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Fine-tuning:\n",
      "PP = 1.8737\n"
     ]
    }
   ],
   "source": [
    "print(f\"After Fine-tuning:\")  \n",
    "print(f\"PP = {perplexity2:>0.4f}\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
